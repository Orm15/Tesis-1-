\chapter{Desarrollo de la Solución}
Dentro de esta sección se expone con precisión el procedimiento descrito previamente en la sección anterior, correspondiente a cada acción incluida en la metodología implementada, junto con los entregables previstos.

\section{Adquisición de los Datos}

\textbf{Actividad 1: Identificación de la data que contengan imágenes de rostros con características morfológicas faciales de proporciones similares}
 
Con el objetivo de entrenar un modelo de segmentación enfocado en características morfológicas faciales como arrugas y manchas, se llevó a cabo una exhaustiva búsqueda de bases de datos públicas en repositorios especializados. Se priorizó la recolección de conjuntos de datos que incluyeran imágenes faciales en alta resolución, variedad de edades, géneros y tonos de piel, así como condiciones de iluminación lo suficientemente controladas como para facilitar la segmentación de detalles faciales sutiles.

Durante este proceso, se identificaron diversos repositorios en plataformas como GitHub que ofrecían acceso a bases de datos con más de 200,000 imágenes faciales. Entre los conjuntos más destacados se encuentran versiones extendidas de datasets como CelebA, FFHQ (Flickr-Faces-HQ), y otros compendios curados por la comunidad investigadora, que contienen imágenes faciales etiquetadas o anotadas para tareas de reconocimiento y análisis facial.

Sin embargo, debido al enfoque específico de esta investigación la segmentación de características morfológicas finas como arrugas y manchas fue necesario realizar una selección cuidadosa de las imágenes más adecuadas. Para ello, se aplicaron criterios de claridad visual, resolución suficiente y visibilidad explícita de las deformaciones morfológicas faciales. Como resultado de este filtrado, se seleccionó un subconjunto compuesto por 5,000 imágenes faciales que cumplían con los siguientes criterios:

\begin{itemize}
    \item Alta calidad visual.   
    \item Visibilidad clara de texturas de la piel.
    \item Presencia evidente de arrugas y manchas.
    \item Proporciones faciales dentro de un rango estándar para facilitar el modelado.
\end{itemize} 

Este conjunto reducido pero representativo,lo podemos ver en la Figura \ref{4:fig1}, constituye la base de entrenamiento y validación del modelo propuesto. La selección manual de estos datos buscó optimizar el desempeño del modelo, al exponerlo exclusivamente a ejemplos que contienen información útil para aprender patrones asociados a las características morfológicas de interés.

\begin{figure}[h]
	\begin{center}
		\includegraphics[width=0.75\textwidth]{4/figures/data.png}
		\caption[Dataset recolectado de repositorios]{Dataset recolectado de repositorios.\\
		Fuente: Elaboración propia}
		\label{4:fig1}
	\end{center}
\end{figure}

\section{Preprocesamiento de los Datos}

\textbf{Actividad 1: Filtración de imágenes faciales con características morfológicas}

Una vez recopilado el subconjunto de 5,000 imágenes faciales con características morfológicas visibles, se procedió a una etapa de filtración adicional orientada a reforzar la consistencia y la relevancia de los datos para la tarea de segmentación. Esta filtración se enfocó en garantizar que todas las imágenes seleccionadas contuvieran arrugas y manchas claramente identificables, descartando aquellas que, pese a su calidad visual, no presentaban estas características de manera explícita.

El proceso se realizó mediante una inspección semiautomática, apoyada en técnicas básicas de detección de texturas y realce de bordes para facilitar la identificación visual de las zonas con deformaciones. Como resultado, se aseguraron condiciones homogéneas entre las imágenes del dataset final, lo que permitió una base sólida para la generación de máscaras de segmentación precisas.

\textbf{Actividad 2: Representación y normalización de las imágenes faciales}

Con el conjunto de imágenes definitivo, se ejecutó un proceso de preprocesamiento estandarizado con dos objetivos principales: uniformar las dimensiones espaciales de las imágenes y generar las máscaras binarias asociadas a las regiones con deformaciones morfológicas.

En primer lugar, todas las imágenes fueron redimensionadas a una resolución uniforme de 1024x1024 píxeles, preservando la relación de aspecto y aplicando interpolación bilineal para mantener la calidad visual. Esta estandarización es fundamental para asegurar la compatibilidad estructural con las redes neuronales convolucionales utilizadas en etapas posteriores, y facilita la aplicación de operaciones convolucionales sobre áreas homogéneas.

En segundo lugar, se generaron máscaras binarias (en blanco y negro) correspondientes a cada imagen. Estas máscaras representan las regiones específicas del rostro que contienen arrugas, manchas u otras alteraciones morfológicas, codificadas de la siguiente manera:

\begin{itemize}
    \item Blanco (valor 1): Regiones con características morfológicas relevantes (objetivo de segmentación).   
    \item Negro (valor 0): Regiones sin interés morfológico.
\end{itemize} 

Las máscaras, como se ve en la Figura \ref{4:fig2}, fueron elaboradas a partir de una combinación de anotación semiautomática y herramientas de realce de texturas, contrastes y gradientes, con verificación manual en una muestra aleatoria para asegurar su validez.

\begin{figure}[h]
	\begin{center}
		\includegraphics[width=0.75\textwidth]{4/figures/mascaras.png}
		\caption[Máscaras binarias generadas]{Máscaras binarias generadas.\\
		Fuente: Elaboración propia}
		\label{4:fig2}
	\end{center}
\end{figure}

Este proceso de representación y normalización, como se ve en el diagrama final de Preprocesamiento en la Figura \ref{4:fig3}, permitió convertir los datos originales en pares de entrada (imagen facial) y salida (máscara de segmentación), aptos para el entrenamiento supervisado del modelo de segmentación basado en redes neuronales convolucionales.

\begin{figure}[h]
	\begin{center}
		\includegraphics[width=0.45\textwidth]{4/figures/diagrama final prepo.png}
		\caption[Diagrama de Preprocesamiento Utilizado]{Diagrama de Preprocesamiento Utilizado.\\
		Fuente: Elaboración propia}
		\label{4:fig2}
	\end{center}
\end{figure}

\section{Desarrollo de los Modelos de Segmentación}

\subsection{Modelo de Segmentación: Unet}

\subsubsection{Diseño de la arquitectura del modelo}

El modelo implementado es una arquitectura de red neuronal convolucional U-Net, que es comúnmente utilizada para tareas de segmentación de imágenes. Esta red se caracteriza por su estructura simétrica, compuesta por una parte de codificación (encoder) y decodificación (decoder), con conexiones de salto (skip connections) entre ellas. El objetivo principal de la U-Net es capturar características de bajo y alto nivel, y luego reconstruir la imagen segmentada a partir de esas características.

\textbf{Estructura del Modelo}

La arquitectura de la U-Net se puede dividir en dos fases principales: la fase de codificación (encoder) y la fase de decodificación (decoder).

\textbf{Bloques de Convolución}

Cada bloque de convolución en el encoder y el decoder sigue el mismo patrón. Un bloque está compuesto por:
\begin{itemize}
    \item Dos capas de convolución, cada una seguida por \texttt{BatchNorm2d} (normalización por lotes) y \texttt{ReLU} (función de activación).
    \item La normalización por lotes ayuda a estabilizar y acelerar el entrenamiento, mientras que la activación ReLU introduce no linealidad al modelo.
    \item Las capas de convolución utilizan un tamaño de filtro de $3 \times 3$, lo que significa que cada capa de convolución opera sobre una vecindad local de 3 píxeles en la imagen de entrada.
\end{itemize}

\textbf{Encoder (Codificador)}

El encoder está diseñado para extraer características de la imagen a través de una serie de capas convolucionales. En cada paso, la imagen es reducida en resolución mediante el uso de max pooling, lo que permite que la red capture información contextual en escalas de mayor nivel.

\begin{itemize}
    \item \textbf{Encoder 1}: La imagen de entrada con 3 canales (RGB) pasa a través del primer bloque de convolución, produciendo 64 mapas de características. Luego, se aplica un max pooling para reducir la resolución de la imagen a la mitad.
    \item \textbf{Encoder 2}: Los 64 mapas de características se procesan en el segundo bloque de convolución, produciendo 128 mapas de características. También se aplica max pooling para reducir la resolución.
    \item \textbf{Encoder 3}: Los 128 mapas de características se procesan en el tercer bloque de convolución, produciendo 256 mapas de características, seguidos por max pooling.
    \item \textbf{Encoder 4}: Los 256 mapas de características se procesan en el cuarto bloque de convolución, produciendo 512 mapas de características, seguidos por max pooling.
\end{itemize}

\textbf{Bottleneck}

En esta etapa, la red alcanza su máxima abstracción. El bottleneck se compone de un bloque de convolución que aumenta la profundidad a 1024 mapas de características. A diferencia de los bloques anteriores, esta parte no aplica max pooling, pero se realiza una convolución estándar. Aquí, la resolución de la imagen se mantiene constante para preservar la información de alto nivel.

\textbf{Decoder (Decodificador)}

El decoder tiene como objetivo reconstruir la imagen segmentada a partir de la representación abstracta generada en el bottleneck. El proceso se realiza mediante convoluciones transpuestas (también conocidas como deconvoluciones), que aumentan la resolución de las características. Además, se utilizan conexiones de salto (skip connections) para combinar la información de alta resolución del encoder con la información generada en el decoder.

\begin{itemize}
    \item \textbf{Upconv4 y Decoder 4}: La salida del bottleneck (1024 mapas de características) se expande utilizando una convolución transpuesta (upconv4), lo que duplica la resolución de la imagen. Luego, esta salida se concatena con los 512 mapas de características de \texttt{enc4}. El resultado se pasa a través de un bloque convolucional (dec4), manteniendo la cantidad de mapas en 512.
    \item \textbf{Upconv3 y Decoder 3}: De manera similar, se aplica una convolución transpuesta (upconv3) que aumenta la resolución de la imagen a la mitad de su tamaño original. Se concatena con los 256 mapas de características de \texttt{enc3} y se pasa por un bloque convolucional (dec3).
    \item \textbf{Upconv2 y Decoder 2}: La resolución de la imagen se duplica nuevamente usando upconv2, y se concatena con los 128 mapas de características de \texttt{enc2}. El resultado se procesa en dec2.
    \item \textbf{Upconv1 y Decoder 1}: Finalmente, se utiliza upconv1 para duplicar la resolución a su tamaño original. Los 64 mapas de características generados en \texttt{enc1} se concatenan con la salida de upconv1 y se pasan a través de dec1.
\end{itemize}

\textbf{Capa Final}

La salida final de la red se genera aplicando una convolución $1 \times 1$. Esta capa reduce el número de canales de 64 a la cantidad deseada de canales en la salida (por ejemplo, 3 para una imagen RGB segmentada). La convolución de $1 \times 1$ permite realizar una clasificación de cada píxel de la imagen.

\textbf{Funcionamiento General}

\begin{enumerate}
    \item La imagen de entrada pasa por el encoder, donde se extraen características a diferentes escalas. La resolución de la imagen se reduce en cada paso utilizando max pooling, y la profundidad de los mapas de características aumenta.
    \item En el bottleneck, se extrae la representación más abstracta de la imagen, utilizando un bloque convolucional sin max pooling.
    \item El decoder aumenta la resolución de la imagen utilizando convoluciones transpuestas, y las conexiones de salto (skip connections) se utilizan para combinar las características de bajo nivel del encoder con las características de alto nivel del decoder.
    \item Finalmente, una capa de convolución $1 \times 1$ genera la salida segmentada de la red.
\end{enumerate}

\textbf{Pseudocódigo del Modelo}

\begin{verbatim}
Input: Imagen X (3 canales)

e1 = enc1(X)  # Primer bloque de convolución
e2 = enc2(pool1(e1))  # Segundo bloque de convolución
e3 = enc3(pool2(e2))  # Tercer bloque de convolución
e4 = enc4(pool3(e3))  # Cuarto bloque de convolución

b = bottleneck(pool4(e4))  # Bottleneck, no max pooling

d4 = upconv4(b)  # Expansión con convolución transpuesta
d4 = dec4(concat(d4, e4))  # Conexión de salto con e4
d3 = upconv3(d4)  # Expansión con convolución transpuesta
d3 = dec3(concat(d3, e3))  # Conexión de salto con e3
d2 = upconv2(d3)  # Expansión con convolución transpuesta
d2 = dec2(concat(d2, e2))  # Conexión de salto con e2
d1 = upconv1(d2)  # Expansión con convolución transpuesta
d1 = dec1(concat(d1, e1))  # Conexión de salto con e1

Output = final_conv(d1)  # Capa final de convolución
\end{verbatim}

\subsubsection{Definición de componentes del modelo}

\begin{itemize}
    \item \textbf{Clase \texttt{SegmentacionMulticlaseDataset}}: Esta clase personalizada hereda de \texttt{torch.utils.data.Dataset} y se encarga de cargar imágenes de entrada (rostros) y sus correspondientes máscaras binarias (arrugas y manchas). Se generan máscaras multiclase fusionando las siguientes clases:
    \begin{itemize}
        \item Clase 0: Fondo (valor por defecto en la máscara).
        \item Clase 1: Arrugas (píxeles donde la imagen de arrugas tenga valores mayores a 50).
        \item Clase 2: Manchas (píxeles donde la imagen de manchas tenga valores mayores a 50).
    \end{itemize}
    Para el preprocesamiento se utiliza la librería Albumentations. Las transformaciones incluyen:
    \begin{itemize}
        \item \texttt{HorizontalFlip(p=0.5)}
        \item \texttt{Rotate(limit=10, p=0.3)}
        \item \texttt{RandomBrightnessContrast(p=0.3)}
        \item \texttt{Resize(256, 256)}
        \item \texttt{ToTensorV2()}
    \end{itemize}
    Esto permite robustecer el modelo contra pequeñas variaciones geométricas e iluminación.

    \item \textbf{Modelo \texttt{UNet}}: Arquitectura de segmentación semántica basada en una estructura en forma de U. Se compone de dos fases principales:
    \begin{itemize}
        \item \textbf{Codificador (Encoder):} Reduce progresivamente la dimensión espacial mientras aumenta la profundidad de características:
        \begin{itemize}
            \item \texttt{enc1}: \( \text{Conv2d}(3 \rightarrow 64) \rightarrow \text{BN} \rightarrow \text{ReLU} \rightarrow \text{Conv2d}(64 \rightarrow 64) \rightarrow \text{BN} \rightarrow \text{ReLU} \)
            \item \texttt{pool1}: MaxPool \(2 \times 2\)
            \item \texttt{enc2}: \(64 \rightarrow 128\), seguido de \texttt{pool2}, etc.
        \end{itemize}
        \item \textbf{Cuello de botella (Bottleneck):} Capa de mayor profundidad \(512 \rightarrow 1024\), sin reducción espacial.
        \item \textbf{Decodificador (Decoder):} Recupera resolución espacial mediante \texttt{ConvTranspose2d} (upsampling) y concatenación con las salidas del encoder (skip connections). Cada bloque incluye dos capas convolucionales como el encoder.
        \item \textbf{Capa final:} Una convolución \(1 \times 1\) que transforma el mapa de características final (con 64 canales) en un tensor con 3 canales de salida (uno por clase).
    \end{itemize}
\end{itemize}

\vspace{1em}
\textbf{Componentes de entrenamiento:}

\begin{itemize}
    \item \textbf{Dispositivo de cómputo:} Se selecciona automáticamente el uso de GPU con CUDA, 8 GB de Ram con tarjeta de video RTX 4060
    \[
    \texttt{torch.device("cuda" if torch.cuda.is\_available() else "cpu")}
    \]
    
    \item \textbf{Hiperparámetros:}
    \begin{itemize}
        \item Épocas: \( E = 50 \)
        \item Tamaño de lote: \( B = 4 \)
        \item Tasa de aprendizaje: \( \eta = 10^{-3} \)
        \item Número de clases: \( C = 3 \)
    \end{itemize}

    \item \textbf{Función de pérdida: CrossEntropy ponderada}

    Se utiliza:
    \[
    \mathcal{L}_{CE}(x, y) = - \sum_{c=1}^{C} w_c \cdot y_c \cdot \log \left( \frac{e^{x_c}}{\sum_{k=1}^{C} e^{x_k}} \right)
    \]
    Donde:
    \begin{itemize}
        \item \( x \) es la salida del modelo (logits por clase, por píxel),
        \item \( y \) es la máscara real codificada como entero [0, 1, 2],
        \item \( w_c \in \mathbb{R} \) es el peso de la clase \(c\).
    \end{itemize}
    Se definen pesos:
    \[
    \vec{w} = [0.5, 5.0, 3.0]
    \]
    para compensar el desbalance en la presencia de fondo, arrugas y manchas respectivamente.

    \item \textbf{Optimización:} Se utiliza Adam:
    \[
    \theta \leftarrow \theta - \eta \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
    \]
    con momentos corregidos, donde \(\theta\) representa los parámetros de la red.

    \item \textbf{Bucle de entrenamiento:} Para cada lote:
    \begin{enumerate}
        \item Se pasa la imagen por el modelo: \( \hat{y} = f_\theta(x) \)
        \item Se calcula la pérdida: \( \mathcal{L} = \text{CrossEntropy}(\hat{y}, y) \)
        \item Se retropropaga: \( \nabla_\theta \mathcal{L} \)
        \item Se actualizan los parámetros usando Adam.
    \end{enumerate}

    \item \textbf{Evaluación:} Se obtuvo la clase predicha para cada píxel:
    \[
    \hat{y}_{pred}(i,j) = \arg\max_{c \in \{0,1,2\}} \hat{y}_{c}(i,j)
    \]
    y se imprimen las clases únicas predichas y reales, para comparar semánticamente la segmentación.
\end{itemize}

\vspace{1em}
\textbf{Resumen de interacción entre componentes:}

\begin{itemize}
    \item El conjunto de datos genera muestras de entrenamiento y validación a partir de rutas de imágenes faciales y sus respectivas máscaras binarias.
    \item Las imágenes se transforman y normalizan en el rango \([0, 1]\) y se convierten en tensores \texttt{float}.
    \item La máscara multiclase (valores en \{0,1,2\}) es convertida en tipo \texttt{long} para ser compatible con \texttt{nn.CrossEntropyLoss}.
    \item El modelo UNet recibe un batch de imágenes y predice un tensor de salida con forma \((B, C, H, W)\), donde \(C = 3\).
    \item Se calcula la pérdida ponderada y se retropropaga para actualizar los pesos.
    \item En validación, se observa la capacidad del modelo para distinguir fondo, arrugas y manchas en imágenes no vistas.
\end{itemize}

\subsubsection{Entrenamiento del modelo}
\textbf{Actividad 1: Configuración del entorno de entrenamiento}

La configuración del entorno es esencial para garantizar la reproducibilidad y eficiencia del proceso de entrenamiento del modelo. Se utilizó Python 3.10 con las siguientes librerías principales: PyTorch 2.0.0, Albumentations, OpenCV y CUDA/cuDNN para aceleración por GPU si está disponible.

\begin{itemize}
  \item \textbf{Detección del dispositivo:}\\
  \texttt{device = torch.device("cuda" if torch.cuda.is\_available() else "cpu")}
  
  \item \textbf{Estructura de datos:}
  \begin{itemize}
    \item \texttt{rostros/} -- imágenes RGB del rostro completo.
    \item \texttt{arrugas/} -- máscaras binarias para arrugas.
    \item \texttt{manchas/} -- máscaras binarias para manchas.
  \end{itemize}

  \item \textbf{Transformaciones con Albumentations:}
\begin{verbatim}
A.Compose([
    A.HorizontalFlip(p=0.5),
    A.Rotate(limit=10, p=0.3),
    A.RandomBrightnessContrast(p=0.3),
    A.Resize(256, 256),
    ToTensorV2()
])
\end{verbatim}

  \item \textbf{Dataset personalizado:} \\
  Se creó una clase que transforma las máscaras binarias individuales en una sola máscara multiclase: 0 para fondo, 1 para arrugas y 2 para manchas.

  \item \textbf{División de datos:} \\
  Se utilizó \texttt{random\_split} para una partición del 80\% para entrenamiento y 20\% para validación.
\end{itemize}

\vspace{0.5cm}
\textbf{Actividad 2: Aplicación de técnicas de optimización}

Durante el entrenamiento de la red UNet, se implementaron diversas técnicas para mejorar la precisión y velocidad de convergencia del modelo.

\begin{itemize}
  \item \textbf{Modelo UNet:}
  \begin{itemize}
    \item Capas convolucionales con BatchNorm y activación ReLU.
    \item MaxPooling y transposed convolutions.
    \item Conexiones de salto entre codificador y decodificador.
  \end{itemize}
  
  \item \textbf{Función de pérdida:} \\
  Se utilizó entropía cruzada ponderada para manejar el desbalance de clases:
  \[
  \text{weights} = [0.5, 5.0, 3.0]
  \]
  \[
  \mathcal{L}_{CE} = - \sum_{i=1}^{N} \sum_{c=1}^{C} w_c \cdot y_{ic} \cdot \log \left( \frac{e^{x_{ic}}}{\sum_{k=1}^{C} e^{x_{ik}}} \right)
  \]

  \item \textbf{Optimizador:} \\
  Se empleó Adam con tasa de aprendizaje $\eta = 0.001$:
  \[
  \theta_{t+1} = \theta_t - \eta \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
  \]

  \item \textbf{Proceso de entrenamiento:}
  \begin{itemize}
    \item 30 épocas con batch size = 4.
    \item Cálculo de predicción, pérdida, retropropagación y optimización en cada iteración.
  \end{itemize}

  \item \textbf{Otras técnicas aplicadas:}
  \begin{itemize}
    \item Aumento de datos en tiempo real.
    \item Normalización por lotes.
    \item Conexiones de salto para evitar pérdida de detalle espacial.
  \end{itemize}
\end{itemize}

\vspace{0.5cm}
\textbf{Actividad 3: Validación cruzada del rendimiento}

Se aplicó una estrategia de validación tipo hold-out (80\% entrenamiento, 20\% validación). El objetivo fue medir la capacidad del modelo para generalizar a datos no vistos.

\begin{itemize}
  \item \textbf{Métricas utilizadas:}
  \begin{itemize}
    \item Precisión por clase.
    \item Intersección sobre Unión (IoU).
    \item Dice Coefficient:
    \[
    \text{Dice} = \frac{2 \cdot |A \cap B|}{|A| + |B|}
    \]
    \item Matriz de confusión multiclase.
  \end{itemize}

  \item \textbf{Visualización de resultados:} \\
  Las máscaras predichas se superpusieron con las imágenes originales utilizando diferentes colores por clase (fondo, arrugas, manchas) para evaluar visualmente la segmentación.

  \item \textbf{Extensiones propuestas:}
  \begin{itemize}
    \item Validación cruzada k-fold ($k=5$).
    \item Early stopping para prevenir sobreajuste.
    \item Reducción adaptativa de la tasa de aprendizaje.
    \item Ensembling de modelos.
  \end{itemize}

  \item \textbf{Justificación:} \\
  Una evaluación rigurosa permite determinar si el modelo puede ser aplicado de forma confiable a nuevas imágenes faciales con características morfológicas diversas.
\end{itemize}

\subsubsection{Evaluación del modelo}
La evaluación del modelo entrenado se realizó con el objetivo de determinar su capacidad para segmentar correctamente las características morfológicas faciales, específicamente arrugas y manchas, a partir de imágenes RGB. Esta etapa es fundamental para verificar la efectividad del modelo antes de su implementación práctica en un entorno real.

\begin{itemize}
  \item \textbf{Conjunto de validación:}\\
  Se utilizó el 20\% de los datos reservados previamente para validación, asegurando que las imágenes evaluadas no hayan sido vistas durante el entrenamiento. Esto garantiza una estimación imparcial del rendimiento general del modelo.

  \item \textbf{Métricas de rendimiento:}\\
  Para una evaluación integral del modelo, se utilizaron diversas métricas que capturan distintos aspectos de su desempeño:
  \begin{itemize}
    \item \textbf{Precisión por clase (accuracy):} proporción de píxeles correctamente clasificados.
    \item \textbf{IoU (Intersection over Union):}
    \[
    \text{IoU}_c = \frac{|P_c \cap G_c|}{|P_c \cup G_c|}
    \]
    donde $P_c$ representa los píxeles predichos de la clase $c$, y $G_c$ los píxeles reales de dicha clase.

    \item \textbf{Coeficiente Dice:}
    \[
    \text{Dice}_c = \frac{2 \cdot |P_c \cap G_c|}{|P_c| + |G_c|}
    \]
    especialmente útil para evaluar el solapamiento entre predicción y verdad de terreno en clases minoritarias como las manchas.

  \end{itemize}

  \item \textbf{Análisis Visual:}\\
  Además de las métricas cuantitativas, se realizó una revisión visual de los resultados generados por el modelo. Para ello:
  \begin{itemize}
    \item Se superpusieron las máscaras predichas sobre las imágenes originales.
    \item Se utilizó una codificación de colores: fondo (negro), arrugas (rojo) y manchas (azul).
    \item Se compararon visualmente las segmentaciones predichas frente a las segmentaciones reales (ground truth).
  \end{itemize}

  \item \textbf{Observaciones esperadas:}
  \begin{itemize}
    \item Buena segmentación de estructuras prominentes (arrugas marcadas).
    \item Posibles errores en bordes difusos o regiones con baja iluminación.
    \item Posibles confusiones entre arrugas y manchas en regiones solapadas.
  \end{itemize}

  \item \textbf{resultados:}\\
El modelo alcanzó los siguientes resultados promedio sobre el conjunto de validación:
\begin{itemize}
  \item \textbf{Pérdida mínima (\texttt{val\_loss}):} 0.0849 (en la época 50).
  \item \textbf{Precisión promedio:} 0.645, indicando que casi dos tercios de los píxeles se clasificaron correctamente.
  \item \textbf{Dice Score promedio:} 0.440, mostrando un nivel medio de superposición entre máscaras predichas y reales.
  \item \textbf{Jaccard Index promedio:} 0.289, reflejando el desafío de obtener coincidencias exactas en regiones complejas.
  \item Se detectaron mejores resultados en la clase \emph{arrugas}, posiblemente por la mayor representación y contraste de textura.
  \item La clase \emph{manchas} presentó métricas ligeramente inferiores, sugiriendo que futuras optimizaciones deberían incluir aumentos más dirigidos a mejorar esta categoría (por ejemplo, resaltando contraste o coloración).
\end{itemize}
  A continuación se presentan las métricas calculadas, los gráficos obtenidos y los ejemplos visuales para ilustrar el rendimiento del modelo.

  \vspace{0.2cm}
  \begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{4/figures/UnetComparación1.png}
\caption{Comparación visual: imagen original, máscara real multicategoría y predicción del modelo para un caso con predominio de arrugas.}
\label{fig:validacion11}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{4/figures/UnetComparación2.png}
\caption{Comparación visual: ejemplo donde se observa el desempeño del modelo en la detección de manchas, destacando regiones correctamente identificadas y algunas áreas faltantes.}
\label{fig:validacion22}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{4/figures/UnetComparación3.png}
\caption{Comparación visual: caso mixto donde se presentan simultáneamente arrugas y manchas, mostrando la capacidad del modelo para diferenciar ambas clases en un mismo rostro.}
\label{fig:validacion33}
\end{figure}
\end{itemize}


\subsection{Modelo de Segmentación: UNetAttention}

\subsubsection{Diseño de la arquitectura del modelo}


La arquitectura propuesta se basa en el modelo \textbf{U-Net con mecanismos de atención} (\texttt{UNetAttention}), que combina una red convolucional simétrica tipo encoder-decoder con puertas de atención (\texttt{AttentionGate}) para mejorar el enfoque en las regiones relevantes de la imagen.

\textbf{Encoder (Codificador)}
El encoder está diseñado para capturar características de bajo a alto nivel a partir de la imagen de entrada. Consiste en cuatro bloques llamados \texttt{DoubleConv}:
\begin{itemize}
    \item \textbf{Encoder 1}: Dos convoluciones $3\times3$ (con \texttt{padding}=1), normalización por lotes (BatchNorm) y activación ReLU. Extrae características básicas (bordes, texturas) de las entradas con 3 canales (RGB) y produce 64 mapas de características.
    \item \textbf{Encoder 2}: Similar al anterior, pero recibe 64 mapas y genera 128, aumentando la capacidad de representación.
    \item \textbf{Encoder 3}: Pasa de 128 a 256 mapas, detectando patrones más complejos.
    \item \textbf{Encoder 4}: Pasa de 256 a 512 mapas, capturando características globales.
\end{itemize}
Cada bloque está seguido de una capa \texttt{MaxPool2d(2)}, que reduce la resolución a la mitad e incrementa el campo receptivo, permitiendo captar relaciones espaciales más amplias.

\textbf{Puente (Bottleneck)}
Entre el encoder y el decoder se encuentra el \textbf{bottleneck}, otra capa \texttt{DoubleConv} que recibe 512 mapas y genera 1024. Esta parte actúa como un resumen altamente condensado de toda la información, crucial para sintetizar las representaciones aprendidas antes de reconstruirlas.

\textbf{Attention Gates (Puertas de Atención)}
En lugar de pasar directamente los mapas del encoder al decoder (skip connections), se introducen mecanismos de atención:
\begin{itemize}
    \item Cada \texttt{AttentionGate} recibe dos entradas: $g$ (señal del decoder) y $x$ (señal del encoder).
    \item Ambas se alinean en dimensiones espaciales y se transforman con convoluciones $1\times1$ para reducir canales.
    \item La suma de ambas se activa con ReLU, pasa por otra convolución $1\times1$, BatchNorm y una función sigmoide, generando un mapa de atención $\psi$ entre 0 y 1.
    \item Finalmente, $\psi$ se multiplica por la señal $x$ para filtrar solo las características relevantes.
\end{itemize}
Esto asegura que solo la información útil del encoder fluya al decoder, reduciendo el ruido y enfocándose en las regiones clave.

\textbf{Decoder (Decodificador)}
El decoder reconstruye la imagen segmentada paso a paso:
\begin{itemize}
    \item \textbf{Upconv4}: Upsampling (\texttt{ConvTranspose2d}) de 1024 a 512, concatenado con la salida del encoder4 filtrado por la atención (att3).
    \item \textbf{Decoder4}: \texttt{DoubleConv} que procesa los 1024 canales concatenados y produce 512.
    \item \textbf{Upconv3}, \textbf{Decoder3}: De 512 a 256, concatenado con att2, procesado a 256.
    \item \textbf{Upconv2}, \textbf{Decoder2}: De 256 a 128, concatenado con att1, procesado a 128.
    \item \textbf{Upconv1}, \textbf{Decoder1}: De 128 a 64, concatenado con enc1 (sin atención), procesado a 64.
\end{itemize}

\textbf{Capa Final}
La capa final es una convolución $1\times1$ que reduce los 64 mapas de características a un solo canal de salida (\texttt{out\_channels=1}), generando el mapa binario segmentado.

\textbf{Razones del Diseño}
\begin{itemize}
    \item \textbf{Convoluciones dobles}: Mejoran la capacidad de aprendizaje local sin aumentar demasiado la profundidad.
    \item \textbf{BatchNorm}: Acelera la convergencia y estabiliza el entrenamiento.
    \item \textbf{Atención}: Permite que el modelo se enfoque en las regiones importantes, evitando pasar ruido del encoder al decoder.
    \item \textbf{Skip connections}: Ayudan a recuperar detalles espaciales perdidos por el pooling.
    \item \textbf{Puertas de atención}: Añaden un mecanismo de filtrado adaptativo, crucial en tareas donde no toda la información del encoder es igualmente relevante.
    \item \textbf{Upsampling con transposed convolutions}: Permite recuperar resolución espacial de manera más precisa que el simple escalado.
\end{itemize}

\textbf{Detalle del Funcionamiento}
El proceso completo del modelo se detalla de la siguiente manera:
\begin{enumerate}
\item La imagen de entrada se procesa a través del encoder, extrayendo características jerárquicas que van desde bordes simples hasta patrones complejos, reduciendo la resolución pero aumentando la abstracción.
\item El bottleneck actúa como un cuello de botella que condensa toda la información del encoder en una representación compacta y de alto nivel.
\item El decoder empieza a recuperar la resolución original usando operaciones de upsampling, reconstruyendo progresivamente el mapa segmentado.
\item En cada paso del decoder, las características recuperadas se enriquecen mediante la concatenación de información relevante proveniente del encoder, filtrada cuidadosamente por las puertas de atención.
\item Las puertas de atención permiten que el modelo ignore información irrelevante y se enfoque en regiones específicas de interés para la tarea de segmentación.
\item Finalmente, la capa de salida transforma las características finales en una máscara binaria que identifica las regiones segmentadas de interés en la imagen.
\end{enumerate}

Este funcionamiento asegura que el modelo combine detalles locales finos con contexto global y mecanismos de relevancia, logrando una segmentación precisa y robusta.

\textbf{Pseudocódigo del Modelo}
\begin{verbatim}
Input: Imagen X

# Encoder
enc1 = DoubleConv(X)
enc2 = DoubleConv(MaxPool(enc1))
enc3 = DoubleConv(MaxPool(enc2))
enc4 = DoubleConv(MaxPool(enc3))

# Bottleneck
bottleneck = DoubleConv(MaxPool(enc4))

# Decoder con atención
up4 = UpConv(bottleneck)
att3 = AttentionGate(up4, enc4)
up4_cat = Concatenate(up4, att3)
dec4 = DoubleConv(up4_cat)

up3 = UpConv(dec4)
att2 = AttentionGate(up3, enc3)
up3_cat = Concatenate(up3, att2)
dec3 = DoubleConv(up3_cat)

up2 = UpConv(dec3)
att1 = AttentionGate(up2, enc2)
up2_cat = Concatenate(up2, att1)
dec2 = DoubleConv(up2_cat)

up1 = UpConv(dec2)
up1_cat = Concatenate(up1, enc1)
dec1 = DoubleConv(up1_cat)

# Salida final
Output = Conv1x1(dec1)
Return Output
\end{verbatim}

\subsubsection{Definición de componentes del modelo}
En esta actividad se definen los módulos fundamentales y los elementos de entrenamiento que constituyen el modelo UNetAttention. A continuación se describe en detalle cada uno:

\textbf{1. Módulos arquitectónicos}

\begin{itemize}
\item \textbf{DoubleConv}:
Es un bloque formado por dos capas convolucionales secuenciales, cada una seguida de Batch Normalization y activación ReLU.
Este bloque permite:
\begin{itemize}
\item Extraer patrones locales en múltiples niveles.
\item Evitar problemas de saturación o desaparición de gradientes gracias a ReLU.
\item Estabilizar y acelerar el entrenamiento mediante BatchNorm, que normaliza las salidas y reduce la dependencia de la inicialización.
\end{itemize}

\item \textbf{AttentionGate}:
Es un módulo que implementa un mecanismo de atención espacial. Su estructura:
\begin{itemize}
\item Dos entradas: 
$\mathbf{x}$ (del encoder) y $\mathbf{g}$ (del decoder)

g (del decoder).
\item Aplicación de convoluciones 
1
×
1
1×1 para reducir dimensionalidad.
\item Suma de las dos señales transformadas y paso por activación ReLU.
\item Otra convolución 
1
×
1
1×1 seguida de BatchNorm y activación Sigmoid, que genera un mapa de atención 
\item Multiplicación punto a punto de $\psi$ con $\mathbf{x}$, filtrando las características relevantes.

x, filtrando las características relevantes.
\end{itemize}
El objetivo es permitir al modelo “enfocarse” solo en las regiones de interés para la tarea de segmentación, reduciendo ruido y mejorando la precisión espacial.

\item \textbf{UpConv (Transposed Convolution)}:
Operación que realiza upsampling aprendido, permitiendo reconstruir la resolución espacial en el decoder. Es superior al upsampling por interpolación porque también aprende qué características son importantes al expandir.

\item \textbf{Conv1x1 final}:
Capa de convolución 
1
×
1
1×1 que reduce los canales finales a uno solo (para segmentación binaria), generando la máscara final.
\end{itemize}

\textbf{2. Componentes de entrenamiento}
\begin{itemize}
    \item \textbf{Optimizador \texttt{AdamW}}:
    El optimizador \texttt{AdamW} es una variante de \texttt{Adam} que incluye decaimiento de pesos (weight decay) como un término de regularización, lo que ayuda a prevenir el sobreajuste durante el entrenamiento. AdamW es particularmente útil para redes neuronales profundas donde los modelos tienden a tener una gran cantidad de parámetros. A continuación se detallan los elementos clave de \texttt{AdamW}:
    
    \begin{itemize}
        \item \textbf{Tasas de Aprendizaje Adaptativas}: 
        El optimizador AdamW mantiene tasas de aprendizaje adaptativas para cada parámetro, lo que significa que los parámetros que tienen gradientes más grandes recibirán actualizaciones más pequeñas, mientras que los parámetros con gradientes más pequeños recibirán actualizaciones más grandes. Este enfoque permite que el modelo aprenda de manera más eficiente y rápida, sin necesidad de ajustarse manualmente a una tasa de aprendizaje fija para todos los parámetros. 

        \item \textbf{Decaimiento de Pesos Desacoplado}: 
        A diferencia de \texttt{Adam}, donde el decaimiento de pesos (penalización L2) es acoplado a la actualización de los momentos de primer y segundo orden, \texttt{AdamW} desacopla el decaimiento de pesos de la actualización del momento. Esto significa que el decaimiento de pesos se aplica directamente a los parámetros del modelo y no a los momentos, lo que mejora la regularización y permite un mejor control sobre la actualización de los parámetros.

        \item \textbf{Ventajas sobre Adam}: 
        \begin{itemize}
            \item El decaimiento de pesos desacoplado de los momentos mejora la capacidad de generalización del modelo, reduciendo el riesgo de sobreajuste.
            \item El mantenimiento de tasas de aprendizaje adaptativas optimiza la convergencia durante el entrenamiento.
            \item La actualización de parámetros se realiza con base en los gradientes de la función de pérdida, pero también incluye una penalización proporcional al tamaño de los pesos, evitando que crezcan demasiado y afecten negativamente el rendimiento.
        \end{itemize}
        
        La actualización de parámetros en \texttt{AdamW} sigue la siguiente fórmula:
        \[
        \theta_t = \theta_{t-1} - \frac{\eta}{\sqrt{v_t} + \epsilon} m_t - \eta \cdot \lambda \cdot \theta_{t-1}
        \]
        donde:
        \begin{itemize}
            \item \( \theta_t \) es el parámetro actualizado en el paso \( t \),
            \item \( m_t \) es el primer momento (promedio de gradientes),
            \item \( v_t \) es el segundo momento (promedio de los cuadrados de los gradientes),
            \item \( \lambda \) es el factor de decaimiento de pesos (regularización L2),
            \item \( \eta \) es la tasa de aprendizaje.
        \end{itemize}
    \end{itemize}

    \item \textbf{Scheduler \texttt{ReduceLROnPlateau}}:
    El planificador \texttt{ReduceLROnPlateau} ajusta la tasa de aprendizaje durante el entrenamiento en función de una métrica de validación, como la pérdida o precisión de validación. Este planificador reduce la tasa de aprendizaje cuando una métrica deja de mejorar, ayudando a evitar que el optimizador se quede "atascado" en una meseta, lo que puede ocurrir cuando la tasa de aprendizaje es demasiado alta o demasiado baja. A continuación se detallan sus características:
    
    \begin{itemize}
        \item \textbf{Monitoreo de Métrica de Validación}: 
        \texttt{ReduceLROnPlateau} monitoriza una métrica específica de validación, como la pérdida de validación. Cuando esta métrica deja de mejorar durante un número determinado de épocas (\textit{patience}), el planificador reduce la tasa de aprendizaje de manera progresiva.

        \item \textbf{Evitar mesetas en el entrenamiento}: 
        En ocasiones, durante el entrenamiento, el optimizador se puede quedar atrapado en una meseta, donde la métrica de validación se estabiliza y no mejora, aunque el entrenamiento continúe. Reducir la tasa de aprendizaje en estos puntos puede permitir una convergencia más fina y evitar que el modelo se quede estancado en un mínimo local.

        \item \textbf{Convergencia más fina}: 
        A medida que el entrenamiento progresa y el modelo se acerca a una solución óptima, reducir la tasa de aprendizaje permite que el modelo haga ajustes más pequeños pero precisos. Esto es particularmente útil en las etapas finales del entrenamiento, donde la tasa de aprendizaje grande puede causar grandes oscilaciones en los parámetros.

        \item \textbf{Parámetros de Configuración}: 
        Los parámetros clave de \texttt{ReduceLROnPlateau} incluyen:
        \begin{itemize}
            \item \textbf{factor}: Un valor entre 0 y 1 que indica cuánto reducir la tasa de aprendizaje (por ejemplo, 0.1 reducirá la tasa de aprendizaje en un 90\%).
            \item \textbf{patience}: El número de épocas sin mejora antes de que se reduzca la tasa de aprendizaje.
            \item \textbf{min\_lr}: La tasa de aprendizaje mínima que puede alcanzar el optimizador.
        \end{itemize}
    \end{itemize}

    \item \textbf{Función de Pérdida \texttt{CrossEntropyLoss} (con Pesos Balanceados)}:
    La función de pérdida \texttt{CrossEntropyLoss} es adecuada para problemas de clasificación, y se adapta a segmentación de imágenes al tratar cada píxel como una instancia de clasificación. En casos de clases desbalanceadas (como en segmentación de imágenes médicas), se utilizan \textit{pesos balanceados} para abordar este problema. A continuación, se detallan las características de esta función de pérdida:
    
    \begin{itemize}
        \item \textbf{Desbalance de Clases}: 
        En segmentación, es común que las clases estén desbalanceadas, por ejemplo, el fondo puede ocupar la mayor parte de la imagen mientras que las regiones de interés (como una lesión o arruga) son pequeñas. La función \texttt{CrossEntropyLoss} utiliza pesos balanceados para penalizar más fuertemente los errores en las clases minoritarias.
        
        \item \textbf{Cálculo de Pesos Balanceados}:
        Para calcular los pesos de cada clase, se utiliza un enfoque basado en la frecuencia de cada clase en el conjunto de datos. Se emplea el \textit{Median Frequency Balancing}, donde los pesos son calculados en función de la mediana de las frecuencias de aparición de todas las clases en el conjunto de entrenamiento. Este método asegura que las clases menos representadas tengan un mayor peso en la función de pérdida.

        \item \textbf{Impacto en el Entrenamiento}: 
        La asignación de mayores pesos a las clases minoritarias tiene un impacto positivo en la precisión del modelo al hacer que el modelo enfoque su aprendizaje en esas clases más raras. Esto es crucial para la segmentación de regiones pequeñas o poco representadas, como manchas en la piel, lesiones, o áreas específicas de interés en imágenes médicas.

        \item \textbf{Fórmula de la Pérdida}: 
        La fórmula general para la función de pérdida de entropía cruzada con pesos balanceados es:
        \[
        \text{Loss} = - \sum_{i} w_i \cdot y_i \cdot \log(p_i)
        \]
        donde:
        \begin{itemize}
            \item \( y_i \) es la etiqueta verdadera de la clase \( i \),
            \item \( p_i \) es la probabilidad predicha para la clase \( i \),
            \item \( w_i \) es el peso asignado a la clase \( i \) (calculado a partir de la frecuencia).
        \end{itemize}
    \end{itemize}
\end{itemize}

\textbf{3. Resumen de interacción entre componentes}
Durante el entrenamiento:
\begin{enumerate}
\item El modelo recibe imágenes de entrada que son procesadas por los módulos convolucionales y de atención.
\item Las predicciones por píxel son comparadas contra las máscaras reales usando la función de pérdida ponderada.
\item El optimizador AdamW ajusta los pesos del modelo minimizando la pérdida.
\item El scheduler supervisa la evolución de la métrica de validación y adapta la tasa de aprendizaje si es necesario.
\end{enumerate}

Este conjunto de componentes fue seleccionado cuidadosamente para equilibrar capacidad, estabilidad, precisión y generalización.

\subsubsection{Entrenamiento del Modelo}
\textbf{Actividad 1: Configuración del entorno de entrenamiento} \\
Se preparó el entorno de ejecución para garantizar que todos los experimentos fueran reproducibles y consistentes, realizando lo siguiente:
\begin{itemize}
\item Se definieron las rutas de los conjuntos de datos: las carpetas \texttt{face\_images}, \texttt{arrugas} y \texttt{manchas} contenían las imágenes originales y las máscaras de segmentación correspondientes.
\item El tamaño de lote (\emph{batch size}) se fijó en 4, equilibrando el uso eficiente de memoria GPU/CPU y la estabilidad del gradiente durante la retropropagación.
\item Las imágenes de entrada se redimensionaron a 256 $\times$ 256 píxeles para asegurar una entrada uniforme al modelo y reducir el costo computacional.
\item La tasa de aprendizaje inicial se estableció en $1 \times 10^{-4}$, un valor conservador para evitar inestabilidad en las primeras épocas de entrenamiento.
\item Se programaron 50 épocas completas, buscando alcanzar la convergencia del modelo observando las métricas a lo largo del tiempo.
\item Se habilitó el uso del dispositivo CUDA (GPU) cuando estaba disponible, y si no, el entrenamiento se realizaba en CPU.
\item Se fijó una semilla aleatoria global en 42 para garantizar reproducibilidad en todas las operaciones pseudoaleatorias (división de datos, inicialización de pesos, aumentos de datos).
\end{itemize}

\textbf{Actividad 2: Aplicación de técnicas de optimización} \\
Con el objetivo de mejorar la generalización y eficiencia del modelo, se incorporaron varias técnicas de optimización:
\begin{itemize}
\item Se aplicaron aumentos de datos usando la biblioteca \texttt{albumentations}, generando variaciones artificiales en las imágenes para enriquecer el conjunto de entrenamiento y reducir el sobreajuste. Las transformaciones aplicadas fueron:
  \begin{itemize}
  \item \texttt{A.HorizontalFlip(p=0.5)}: aplica volteo horizontal aleatorio con probabilidad 50\%.
  \item \texttt{A.VerticalFlip(p=0.1)}: aplica volteo vertical aleatorio con probabilidad 10\%.
  \item \texttt{A.RandomRotate90(p=0.2)}: rota la imagen aleatoriamente en múltiplos de 90 grados.
  \item \texttt{A.Rotate(limit=15, p=0.3)}: rota la imagen aleatoriamente hasta 15 grados.
  \item \texttt{A.ElasticTransform(alpha=1, sigma=50, approximate=True, p=0.2)}: aplica transformaciones elásticas para simular deformaciones.
  \item \texttt{A.Affine(scale=(0.9, 1.1), translate\_percent=(0.05, 0.05), rotate=(-10, 10), p=0.3)}: realiza escalado, traslación y rotación aleatoria.
  \item \texttt{A.RandomBrightnessContrast(brightness\_limit=0.2, contrast\_limit=0.2, p=0.3)}: modifica aleatoriamente el brillo y contraste.
  \item \texttt{A.GaussNoise(std\_range=(0.2, 0.44), mean\_range=(0, 0), p=0.2)}: añade ruido gaussiano con desviación estándar variable.
  \item \texttt{A.MotionBlur(blur\_limit=3, p=0.1)}: aplica desenfoque de movimiento.
  \item \texttt{A.HueSaturationValue(hue\_shift\_limit=10, sat\_shift\_limit=15, val\_shift\_limit=10, p=0.2)}: ajusta aleatoriamente el tono, la saturación y el valor.
  \item \texttt{A.Resize(256, 256)}: redimensiona la imagen a 256x256 píxeles.
  \item \texttt{ToTensorV2()} : convierte la imagen y máscara en tensores para PyTorch.
  \end{itemize}
  El bloque de código completo utilizado fue:
  \begin{lstlisting}[language=Python]
  train_transforms = A.Compose([
      A.HorizontalFlip(p=0.5),
      A.VerticalFlip(p=0.1),
      A.RandomRotate90(p=0.2),
      A.Rotate(limit=15, p=0.3),
      A.ElasticTransform(alpha=1, sigma=50, approximate=True, p=0.2),
      A.Affine(scale=(0.9, 1.1), translate_percent=(0.05, 0.05), rotate=(-10, 10), p=0.3),
      A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.3),
      A.GaussNoise(std_range=(0.2, 0.44), mean_range=(0, 0), p=0.2),
      A.MotionBlur(blur_limit=3, p=0.1),
      A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=15, val_shift_limit=10, p=0.2),
      A.Resize(256, 256),
      ToTensorV2()
  ])
  \end{lstlisting}

\item Para enfrentar el desbalance entre clases (por ejemplo, menor cantidad de píxeles pertenecientes a manchas comparado con piel), se utilizó el enfoque \emph{median frequency balancing}, que asigna pesos inversamente proporcionales a la frecuencia de cada clase. Estos pesos se pasaron como argumento al criterio \texttt{CrossEntropyLoss}, forzando al modelo a prestar más atención a las clases minoritarias.

\item Se eligió el optimizador \texttt{AdamW}, conocido por su manejo eficiente de regularización y su rendimiento robusto en redes convolucionales, junto con un programador de tasa de aprendizaje (\texttt{ReduceLROnPlateau}), que ajustaba dinámicamente la tasa de aprendizaje cuando la métrica de validación (\texttt{val\_loss}) dejaba de mejorar.

\item Se implementó la impresión detallada de métricas al final de cada época, mostrando pérdidas (\texttt{train\_loss}, \texttt{val\_loss}), precisión, coeficiente Dice e índice de Jaccard, lo cual permitió un seguimiento cuantitativo en tiempo real del progreso del modelo.
\end{itemize}




\textbf{Actividad 3: Validación cruzada del rendimiento} \\
Para evaluar la capacidad del modelo de generalizar sobre datos no vistos, se implementó una rutina robusta de validación:
\begin{itemize}
\item Los datos se dividieron en dos subconjuntos, usando un 80\% para entrenamiento y un 20\% para validación, aplicando la función \texttt{train\_test\_split} para asegurar que ambos conjuntos fueran mutuamente exclusivos.
\item Se construyeron \texttt{DataLoaders} separados para el conjunto de entrenamiento y el de validación, permitiendo un procesamiento eficiente por lotes y garantizando que la evaluación se realizara de forma independiente al entrenamiento.
\item En cada época, se recolectaron todas las predicciones del conjunto de validación y se calcularon métricas de segmentación:
  \begin{itemize}
  \item Precisión global de los píxeles predichos.
  \item Coeficiente Dice, que mide la superposición entre las máscaras predichas y las verdaderas.
  \item Índice de Jaccard, que cuantifica la similitud entre los conjuntos predicho y verdadero.
  \end{itemize}
\item Se guardó automáticamente el modelo correspondiente a la menor pérdida de validación obtenida hasta el momento (\emph{early stopping} implícito), almacenando su estado en el archivo \texttt{best\_model\_Unet\_attention\_100\_epochs.pth}. Esto garantizó que, al final del entrenamiento, se contara con el mejor modelo disponible según el criterio de validación.
\end{itemize}

\subsubsection{Evaluación del Modelo}

\textbf{Actividad 1: Preparación de Datos de Validación} \\
Se prepararon cuidadosamente las imágenes de validación aplicando únicamente transformaciones mínimas para asegurar que las métricas reflejaran el rendimiento real del modelo, sin influencias de aumentos artificiales:
\begin{itemize}
  \item \textbf{Redimensionamiento uniforme:} todas las imágenes y máscaras se reescalaron a 256x256 píxeles, asegurando compatibilidad con las dimensiones de entrada del modelo.
  \item \textbf{Conversión a tensores:} las imágenes se transformaron en tensores (\texttt{ToTensorV2}) sin aplicar alteraciones de brillo, contraste, ruido ni geometría, garantizando que las métricas se calcularan sobre datos reales.
  \item \textbf{Construcción de máscara multicategoría:} las máscaras individuales de arrugas y manchas se combinaron en una única máscara de clase múltiple (con valores codificados por categoría), permitiendo al modelo aprender a distinguir ambas clases en un solo paso de inferencia.
\end{itemize}

\vspace{0.5cm}

\textbf{Actividad 2: Definición de Métricas de Evaluación} \\
Para evaluar de manera robusta el desempeño del modelo, se utilizaron las siguientes métricas:
\begin{itemize}
  \item \textbf{Precisión (macro):} mide la proporción de píxeles predichos correctamente entre todas las clases, promediando por igual sin favorecer a clases más grandes.
  \item \textbf{Dice Score (coeficiente de Sørensen-Dice):} calcula el doble de la intersección dividido por la suma de los tamaños predicho y verdadero, reflejando cuánta superposición real se logró en la segmentación.
  \item \textbf{Índice de Jaccard (IoU):} mide la intersección sobre la unión entre predicciones y referencias, siendo una métrica más estricta para evaluar solapamientos exactos.
\end{itemize}

\vspace{0.5cm}

\textbf{Actividad 3: Evaluación del Modelo} \\
El modelo alcanzó los siguientes resultados promedio sobre el conjunto de validación:
\begin{itemize}
  \item \textbf{Pérdida mínima (\texttt{val\_loss}):} 0.0158 (en la época 50).
  \item \textbf{Precisión promedio:} 0.821, indicando que casi 4 quintos de los píxeles se clasificaron correctamente.
  \item \textbf{Dice Score promedio:} 0.67, mostrando un nivel medio de superposición entre máscaras predichas y reales.
  \item \textbf{Jaccard Index promedio:} 0.61, reflejando aun el desafío de obtener coincidencias exactas en regiones complejas.
  \item Se detectaron mejores resultados en la clase \emph{arrugas}, posiblemente por la mayor representación y contraste de textura.
  \item La clase \emph{manchas} presentó métricas ligeramente inferiores, sugiriendo que futuras optimizaciones deberían incluir aumentos más dirigidos a mejorar esta categoría (por ejemplo, resaltando contraste o coloración).
\end{itemize}

\vspace{0.5cm}

Además de los resultados numéricos, se generaron visualizaciones comparativas entre los datos reales y las predicciones del modelo. Estas imágenes permiten evaluar cualitativamente el desempeño del modelo, mostrando tanto los aciertos como las áreas de mejora. A continuación, se presentan tres ejemplos destacados:

\vspace{0.5cm}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{4/figures/comparación9.png}
\caption{Comparación visual: imagen original, máscara real multicategoría y predicción del modelo para un caso con predominio de arrugas.}
\label{fig:validacion1}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{4/figures/comparación6.png}
\caption{Comparación visual: ejemplo donde se observa el desempeño del modelo en la detección de manchas, destacando regiones correctamente identificadas y algunas áreas faltantes.}
\label{fig:validacion2}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{4/figures/comparación1.png}
\caption{Comparación visual: caso mixto donde se presentan simultáneamente arrugas y manchas, mostrando la capacidad del modelo para diferenciar ambas clases en un mismo rostro.}
\label{fig:validacion3}
\end{figure}


\section{Despliegue}

\textbf{Actividad 1: Preparación del Entorno de Despliegue}

\textbf{Actividad 2: Despliegue del Modelo en Producción}
